{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Requisite Libraries and Initialization\n"
      ],
      "metadata": {
        "id": "pey08tdCrS7B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZyd8TK4p2p8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pyamg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from sklearn.cluster import spectral_clustering\n",
        "import numpy as np\n",
        "\n",
        "from scipy import sparse\n",
        "from scipy.sparse import linalg\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import lil_matrix\n",
        "from scipy.sparse import csgraph\n",
        "from scipy.linalg import fractional_matrix_power\n",
        "\n",
        "import pyamg\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import random\n",
        "\n",
        "from numpy import True_\n",
        "from transformers import FeatureExtractionPipeline, pipeline, RobertaTokenizer, RobertaModel\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "P3grUE0lrbBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change file path in FOLDERNAME variable for new Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "FOLDERNAME = 'curis22'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/"
      ],
      "metadata": {
        "id": "4yD__-0Rrdp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa Pre-processing"
      ],
      "metadata": {
        "id": "YLnsY7u8tSkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopword = stopwords.words('english')\n",
        "stopword.append('=')\n",
        "stopword.append('\\n')\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# feature extraction pipeline\n",
        "pipeline = FeatureExtractionPipeline(model=model, tokenizer=tokenizer, framework=\"pt\")\n",
        "\n",
        "# loads in wikitext dataset\n",
        "dataset = load_dataset(\"wikitext\",  'wikitext-2-raw-v1')\n",
        "ds = dataset['train']"
      ],
      "metadata": {
        "id": "QK3Uj2yotSrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly select n sentences to process from the dataset \"ds\"\n",
        "ds = random.sample(ds, 3000)\n",
        "\n",
        "embeds = []\n",
        "tokens = []\n",
        "\n",
        "for key, sentence in enumerate(ds):\n",
        "  if sentence == '':\n",
        "    continue\n",
        "\n",
        "  # cleaning up the string\n",
        "  sen_ind = 0\n",
        "  sen_split = sentence.split()\n",
        "  sen_split = [i for i in sen_split if i != \"=\"]\n",
        "  length_sen = len(sen_split)\n",
        "  str_to_process = \" \".join(str(x) for x in sen_split)\n",
        "  outputs = pipeline(str_to_process)\n",
        "  outputs = torch.FloatTensor(outputs)\n",
        "  words, features = outputs[0].shape\n",
        "  \n",
        "  sen_mask = torch.zeros(max(0, words), dtype=torch.bool)\n",
        "\n",
        "  while sen_ind < length_sen:\n",
        "    # seeing how many word-grams the current token generates\n",
        "    output = pipeline(sen_split[sen_ind])\n",
        "    output = torch.FloatTensor(output)\n",
        "    num_wordgram, features = outputs[0].shape\n",
        "    \n",
        "    # fill up the corresponding spot in the boolean mask tensor\n",
        "    for i in range(num_wordgram):\n",
        "      if sen_split[sen_ind] in stopword:\n",
        "        sen_mask[i] = False\n",
        "      else:\n",
        "        sen_mask[i] = True\n",
        "    sen_ind += 1\n",
        "\n",
        "  out = outputs[0][sen_mask]\n",
        "  embeds.append(out)\n",
        "\n",
        "  token_ids = tokenizer.__call__(str_to_process)['input_ids']\n",
        "  decoding = tokenizer.batch_decode(token_ids)\n",
        "  tokens.extend(decoding)\n",
        "\n",
        "  if key % 100 == 0: # partially saves embeddings\n",
        "      file1 = open(str('random_word_embeds_wikitext_' + str(key)), 'wb')\n",
        "      pickle.dump(embeds, file1)\n",
        "      file1.close()\n",
        "\n",
        "      file2 = open('random_wiki_tokens_' + str(key)), 'wb')\n",
        "      pickle.dump(tokens, file2)\n",
        "      file2.close()"
      ],
      "metadata": {
        "id": "_LRQ9qOit8l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block converts previously processed embeddings into row/column format suitable for further processing.\n",
        "# Change file names as needed\n",
        "\n",
        "file1 = open('rand_word_embed_wikitext1400' , 'rb')\n",
        "word_embed = pickle.load(file1)\n",
        "file1.close()\n",
        "\n",
        "col = 0\n",
        "discrete_embed = []\n",
        "for i in range(len(word_embed)):\n",
        "  one, num_discrete, dim = word_embed[i].shape\n",
        "  tot_discrete += num_discrete\n",
        "  col = dim\n",
        "  for j in range(num_discrete):\n",
        "    discrete_embed.extend(word_embed[i][0][j])\n",
        "\n",
        "b = torch.Tensor(tot_discrete, dim)\n",
        "torch.stack(discrete_embed, out=b)\n",
        "\n",
        "file2 = open('rand_word_embed_wikitext_1400.pt' , 'wb')\n",
        "pickle.dump(b, file2)\n",
        "file2.close()"
      ],
      "metadata": {
        "id": "VHrXXKthvVvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL token recovery step to get back every token processed to map 1-to-1 with vectors. \n",
        "# Included retroactively in main processing step.\n",
        "\n",
        "file1 = open('random_wiki_sentences_1400', 'rb')\n",
        "selected_sentences = pickle.load( file1)\n",
        "file1.close()\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for key, sentence in enumerate(selected_sentences):\n",
        "  if sentence == '':\n",
        "    continue\n",
        "\n",
        "  # cleaning up the string\n",
        "  sen_ind = 0\n",
        "  sen_split = sentence.split()\n",
        "  sen_split = [i for i in sen_split if i != \"=\"]\n",
        "  length_sen = len(sen_split)\n",
        "  str_to_process = \" \".join(str(x) for x in sen_split)\n",
        "\n",
        "  token_ids = tokenizer.__call__(str_to_process)['input_ids']\n",
        "  decoding = tokenizer.batch_decode(token_ids)\n",
        "  tokens.extend(decoding)\n",
        "  \n",
        "file1 = open('random_wiki_sentences_1400_tokens', 'wb')\n",
        "pickle.dump(tokens, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "6WD8Ea1lvoTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orthogonal Matching Pursuit\n"
      ],
      "metadata": {
        "id": "MdpmktKisoMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "file1 = open('rand_word_embed_wikitext_1400.pt' , 'rb')\n",
        "ds = pickle.load(file1)\n",
        "file1.close()\n",
        "\n",
        "embedding = torch.reshape(ds, (91454, 768))\n",
        "embedding = embedding / np.linalg.norm(embedding, axis = 1)[:,None]\n",
        "embedding.to(device) # code runs on CUDA\n",
        "\n",
        "shape = list(embedding.shape)\n",
        "row, col = shape"
      ],
      "metadata": {
        "id": "8oYaqeySswNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization / hyperparameters: adjust as needed \n",
        "sparsity_low = 80\n",
        "sparsity_med = 120\n",
        "sparsity_high = 250\n",
        "norm_threshold = 0.001\n",
        "norm_threshold_higher = 0.01\n",
        "norm_threshold_high = 0.1\n",
        "norm_threshold_highest = 0.3"
      ],
      "metadata": {
        "id": "ObtRBc1vsyVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cached_index_set = []\n",
        "\n",
        "# change variable names if switching hyperparameters\n",
        "\n",
        "def orthogonal_matching_pursuit_hybrid(input_signal, index):\n",
        "    residual = input_signal\n",
        "    residual.to(device)\n",
        "    num_iters = 0\n",
        "    max_set = [0] * (sparsity_high + 100)\n",
        "    max_set = torch.LongTensor(max_set)\n",
        "    max_set.to(device)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    embed = torch.clone(embedding)\n",
        "    embed[index] = 0\n",
        "\n",
        "    while torch.count_nonzero(max_set) < sparsity_high and (torch.norm(residual) > norm_threshold_highest or num_iters == 0) and i < sparsity_high:\n",
        "        \n",
        "        num_iters += 1\n",
        "        max_set[i] = int(torch.argmax(residual @ embed.T, dim=1).item())\n",
        "        i += 1\n",
        "\n",
        "        residual = input_signal - input_signal @ (torch.linalg.pinv(embedding[max_set[:i]]) @ embedding[max_set[:i]]).T\n",
        "\n",
        "    cached_index_set.append(max_set[:i])\n",
        "    return torch.norm(residual), i"
      ],
      "metadata": {
        "id": "7tRJU5w1soWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = []\n",
        "lengths = []\n",
        "\n",
        "for index in range(0, 91454): # change for-loop range to \"parallelize\" operations on Sherlock\n",
        "    print(index)\n",
        "    res = orthogonal_matching_pursuit_hybrid(embedding[index][None,:], index)\n",
        "    residuals.append(res[0])\n",
        "    lengths.append(res[1])\n",
        "    \n",
        "    if index % 500 == 0:\n",
        "      file1 = open(str('wikitext_id_indices_' + str(index)) , 'wb')\n",
        "      pickle.dump(cached_index_set, file1)\n",
        "      file1.close()\n",
        "\n",
        "      # for fixed index size\n",
        "      file2 = open(str('wikitext_residual_distr_ + str(index)), 'wb')\n",
        "      pickle.dump(residuals, file2)\n",
        "      file2.close()\n",
        "\n",
        "      # for fixed index size\n",
        "      file3 = open(str('wikitext_lengths_distr_' + str(index)), 'wb')\n",
        "      pickle.dump(lengths, file3)\n",
        "      file3.close()"
      ],
      "metadata": {
        "id": "QmJf9o1Ys25F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"Data Processing\" Step after OMP; idiosyncratic to Sherlock batch processing"
      ],
      "metadata": {
        "id": "E873VjJixYj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put all batch files in OMP into one folder and index into it; look through the files to make sure they exist. Might need to remount / refresh Drive.\n",
        "# Code runs through all file names and concatenates intermediate OMP files.\n",
        "# Before Use, rename files. 3 Files from previous pipeline: Index Set, Lengths, Residuals / Appx Error. Compile all 3.\n",
        "\n",
        "file1 = open('/content/drive/My Drive/curis22/rand_word_embed_wikitext_1400.pt' , 'rb')\n",
        "embedding = pickle.load(file1)\n",
        "file1.close()\n",
        "embedding = torch.reshape(embedding, (91454, 768))\n",
        "shape = list(embedding.shape)\n",
        "row, col = shape\n",
        "\n",
        "file_num = 0\n",
        "while file_num <= row:\n",
        "  if path.exists(\"wikitext_lengths_distr_high_res_high_len_rand_\" + str(file_num)) == False:\n",
        "    print(str(file_num) + ' hasn\\'t been added!')\n",
        "  file_num += 500\n",
        "print(file_num)\n",
        "\n",
        "file_num = 0\n",
        "compiled = []\n",
        "\n",
        "while file_num < (90000) - 500:\n",
        "  file = open('wikitext_lengths_distr_high_res_high_len_rand_' + str(file_num), 'rb')\n",
        "  res = pickle.load(file)\n",
        "  file.close()\n",
        "\n",
        "  next_file = open('wikitext_lengths_distr_high_res_high_len_rand_' + str(file_num + 500), 'rb')\n",
        "  next_res = pickle.load(next_file)\n",
        "  next_file.close()\n",
        "\n",
        "  if len(res) < len(next_res): # haven't reached a stopping point yet\n",
        "    file_num += 500\n",
        "    continue\n",
        "\n",
        "  compiled.extend(res) \n",
        "  file_num += 500\n",
        "\n",
        "file1 = open('wikitext_lengths_distr_high_res_high_len_rand_90000', 'rb')\n",
        "last_res = pickle.load(file1)\n",
        "compiled.extend(last_res)\n",
        "\n",
        "file1 = open('wikitext_lengths_distr_high_res_high_len_rand_91454', 'rb')\n",
        "last_res = pickle.load(file1)\n",
        "compiled.extend(last_res)"
      ],
      "metadata": {
        "id": "6TxdQXdOxgIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spectral Clustering"
      ],
      "metadata": {
        "id": "Sn-dIQ2CxFPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open pre=requisite \".pt\" file and list of indices\n",
        "\n",
        "file = open('/home/users/youngch/wiki_ind_highlenres_0.2', 'rb')\n",
        "data_inds = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "file1 = open('rand_word_embed_wikitext_1400.pt' , 'rb')\n",
        "ds = pickle.load(file1)\n",
        "file1.close()\n",
        "\n",
        "whole_embed = torch.reshape(ds, (91454, 768))\n",
        "whole_embed = whole_embed / np.linalg.norm(whole_embed, axis = 1)[:,None]\n",
        "whole_embed.to(device)\n",
        "\n",
        "shape = list(whole_embed.shape)\n",
        "row, col = shape\n",
        "\n",
        "c = lil_matrix((row, row), dtype=np.double)"
      ],
      "metadata": {
        "id": "Ok8Pp2v33KTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ith_dp = processing all linear combinations in data_inds in order, so it's the ith datapoint in our orig. dataset\n",
        "# ind = the indices of the vectors in that element's linear combination\n",
        "for ith_dp, subset in enumerate(data_inds):\n",
        "  # access all the vectors in the index set from the parent embedding\n",
        "  sub_index_set = whole_embed[subset]\n",
        "  sub_index_set.to(device)\n",
        "  # transpose to obtain the right dimensions for matrix multiplication\n",
        "  ci = torch.linalg.pinv(sub_index_set.T) @ whole_embed[ith_dp]\n",
        "  for j, ind in enumerate(subset):\n",
        "    if ind == ith_dp:\n",
        "      print(\"Oh no! An element has its own index in its linear combination!\")\n",
        "    c[ith_dp, int(ind)] = ci[j]\n",
        "\n",
        "W = abs(c) + abs(c.T) # efficient storage\n",
        "one_vec = sparse.eye(row)\n",
        "one_vec = one_vec.diagonal() # efficient storage of 1-vec\n",
        "\n",
        "degree = []\n",
        "for i in range(row):\n",
        "  degree.append(len( np.nonzero(W[i])[0] ))\n",
        "\n",
        "file1 = open('wiki_rand_nonzero_degrees_normalized' , 'wb')\n",
        "pickle.dump(degree, file1)\n",
        "file1.close()\n",
        "\n",
        "W1 = W @ one_vec\n",
        "W1 = sparse.diags(W1)\n",
        "W1 = W1.power(-0.5)\n",
        "L = W1 @ (W @ W1)\n",
        "L = abs(L)\n",
        "\n",
        "file1 = open('wiki_rand_whole_exp_Laplacian_normalized', 'wb')\n",
        "pickle.dump(L, file1)\n",
        "file1.close()\n",
        "\n",
        "file1 = open('wiki_rand_whole_exp_Laplacian_normalized', 'rb')\n",
        "L = pickle.load(file1)\n",
        "file1.close()\n",
        "\n",
        "cluster_num = 1440 # advisory square root of how many points we have\n",
        "resolution_booster = 0\n",
        "\n",
        "labels = spectral_clustering(\n",
        "        L,\n",
        "        eigen_solver = 'amg',\n",
        "        assign_labels = 'cluster_qr',\n",
        "        n_clusters = (cluster_num + resolution_booster),\n",
        "        )\n",
        "\n",
        "# create a dictionary with labels & indices... reevaluate the index set with a new matrix with all elems in the same subspace and look at its rank... \n",
        "cluster_mapping = {}\n",
        "for i, label in enumerate(labels):\n",
        "  if label not in cluster_mapping:\n",
        "    cluster_mapping[label] = [i]\n",
        "  else:\n",
        "    cluster_mapping[label].append(i)\n",
        "\n",
        "file1 = open('wiki_rand_cluster_assignments_normalized' , 'wb')\n",
        "pickle.dump(cluster_mapping, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "tcUgM6izxIdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks = []\n",
        "for i in range(len(cluster_mapping)):\n",
        "  new_mat = whole_embed[cluster_mapping[i]]\n",
        "  rank = np.linalg.matrix_rank(new_mat)\n",
        "  ranks.append(rank)\n",
        "\n",
        "file1 = open('wiki_rand_rank_normalized' , 'wb')\n",
        "pickle.dump(ranks, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "ZTqZEy-u3YoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projections"
      ],
      "metadata": {
        "id": "Ohg2RApsxIoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate effective singular values; graphing subplots is computationally taxing for large number of clusters; consider commenting out\n",
        "\n",
        "# Requires \"labels\" --> cluster assignments from spectral step, load from Drive if needed. \n",
        "\n",
        "# file1 = open('wiki_rand_cluster_assignments_normalized_1440', 'rb')\n",
        "# labels = pickle.load(file1)\n",
        "# file1.close()\n",
        "\n",
        "eff_singular_vals = []\n",
        "percentages = []\n",
        "\n",
        "for key, value in enumerate(labels): \n",
        "  mat = whole_embed[labels[key]]\n",
        "  u, s, vh = torch.linalg.svd(mat)\n",
        "  s = torch.cumsum(s**2 / sum(s**2), dim=0) # normalizing\n",
        "  x_pt = np.linspace(0, 1, len(s))\n",
        "  slopes = (s[1:] - s[:-1]) / x_pt[1] # subtracts consecutive numbers\n",
        "\n",
        "  for ind, slope in enumerate(slopes):\n",
        "    if slopes[ind] <= 0.25:\n",
        "      eff_singular_vals.append(ind + 1)\n",
        "      percentages.append(s[ind + 1])\n",
        "      break\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x_pt, s)\n",
        "  ax.set_title('Singular Value Decomp at Cluster '+ str(key))\n",
        "\n",
        "print(eff_singular_vals)\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(eff_singular_vals)\n",
        "ax.set_title('Overall Effective Ranks')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(percentages)\n",
        "ax.set_title('Percentage Represented')"
      ],
      "metadata": {
        "id": "71CGY0kg5we7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('num_eff_singular_values_rand_1400', 'rb')\n",
        "eff_singular_vals = pickle.load(file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "i6omL3588B9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singular_vector_spaces = []\n",
        "for key, value in enumerate(labels):\n",
        "  mat = whole_embed[labels[key]]\n",
        "  u, s, vh = torch.linalg.svd(mat)\n",
        "  effective_dim_vec = vh[ : eff_singular_vals[key] ][:]\n",
        "  # scale by the singular values, does not work so far\n",
        "  # effective_dim_vec = vh[ : eff_singular_vals[key] ][:] * s[ : eff_singular_vals[key] ][:, None]\n",
        "  singular_vector_spaces.append(effective_dim_vec)"
      ],
      "metadata": {
        "id": "Ij3zVG5t8Cos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def project_input_to_permutation(sample_input):\n",
        "  largest_projections = [] \n",
        "  for i in range(len(singular_vector_spaces)):\n",
        "    proj = (singular_vector_spaces[i] @ sample_input.T)  # projection of k x 768 and 768 x 1\n",
        "    # proj = proj / np.linalg.norm(proj, axis = 1)[:,None]\n",
        "    largest_projections.append( torch.linalg.norm(torch.abs(proj)) ) # appending the number of nonzero values in the projection\n",
        "\n",
        "  largest_projections = torch.tensor(largest_projections)\n",
        "  proj = torch.argmax(largest_projections) # just need the largest, subspace the input is most likely in\n",
        "\n",
        "  subspace_proj = singular_vector_spaces[proj] @ sample_input\n",
        "  subspace_proj = torch.abs(subspace_proj)\n",
        "  subspace_sort = torch.argsort(subspace_proj, descending=True)\n",
        "  subspace_sort = torch.add( subspace_sort, sum(eff_singular_vals[ : max(0, int(proj) - 1)]) )\n",
        "  \n",
        "  # find these k vectors in our space and give them values 1 through k, rest has value 0. Easily change to multiprobe by changing k and num_elem.\n",
        "  num_elem = eff_singular_vals[proj]\n",
        "  perm_vector = torch.zeros(sum(eff_singular_vals))\n",
        "  k = torch.range(1, num_elem)\n",
        "  perm_vector[subspace_sort] = k\n",
        "\n",
        "  return int(proj), torch.as_tensor(perm_vector), largest_projections"
      ],
      "metadata": {
        "id": "tuiUdIrbxLAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of a cluster OOD retrieval task"
      ],
      "metadata": {
        "id": "0kVSzJUE8N6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i1 = whole_embed[2000]\n",
        "proj1, v1, p1 = project_input_to_permutation(i1)\n",
        "print('The current embed is ' + str(tokens[2000]))\n",
        "\n",
        "tokens_in_clus = []\n",
        "for i in range(len(labels[proj1])):\n",
        "  tokens_in_clus.append(tokens[ labels[proj1][i] ])\n",
        "  print(tokens[labels[proj1][i]])\n",
        "print(len(tokens_in_clus))\n",
        "\n",
        "i2 = whole_embed[2000]\n",
        "print('The current embed is ' + str(tokens[2000]))\n",
        "proj2, v2, p2 = project_input_to_permutation(i2)\n",
        "\n",
        "print(v1, v2)\n",
        "print(torch.dot(v1, v2))\n",
        "\n",
        "plt.hist(p1)"
      ],
      "metadata": {
        "id": "vd-Qr3Ma8L6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = 'I try to gauge of the power of women in politics with my own political index' # comparison: \"ACE is , broadly speaking , a measure of the power of the hurricane multiplied by the length of time it existed , so storms that last a long time...\"\n",
        "sen = pipeline(example)\n",
        "sen = torch.FloatTensor(sen)\n",
        "sen = sen / np.linalg.norm(sen, axis = 1)[:,None]\n",
        "x,y,z = sen.shape\n",
        "\n",
        "token_ids = tokenizer.__call__(example)['input_ids']\n",
        "decoding = tokenizer.batch_decode(token_ids)\n",
        "print(decoding)\n",
        "\n",
        "tokens_in_clus = {}\n",
        "perm_vectors = {}\n",
        "for i, word in enumerate(decoding):\n",
        "  if word != '<s>' and word != '</s>':\n",
        "    proj, v, p = project_input_to_permutation(sen[0][i])\n",
        "    words = []\n",
        "    for i in range(len(labels[proj])):\n",
        "      words.append(tokens[labels[proj][i]])\n",
        "    tokens_in_clus[word] = words\n",
        "    perm_vectors[word] = v\n",
        "\n",
        "print(tokens_in_clus)\n",
        "for key, value in enumerate(tokens_in_clus):\n",
        "  print(value, tokens_in_clus[value])\n",
        "\n",
        "print(perm_vectors)\n",
        "\n",
        "# comparing the embeddings for closely across two contexts\n",
        "print(v1)\n",
        "print(perm_vectors[' power'])\n",
        "print(torch.nonzero(v1))\n",
        "print(torch.nonzero(perm_vectors[' power']))\n",
        "print(torch.dot(v1, perm_vectors[' power']))\n",
        "\n",
        "print(tokens_in_clus[' power'])"
      ],
      "metadata": {
        "id": "lx5-DLQA8Rol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking\n"
      ],
      "metadata": {
        "id": "H7DXgnoj8kWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conda install -c conda-forge faiss"
      ],
      "metadata": {
        "id": "TEViA8E-8WcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss"
      ],
      "metadata": {
        "id": "FyKjUH6U8o7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in all relevant files; redirect to personal file path\n",
        "# This does not work on Colab!!!\n",
        "\n",
        "file = open('/Users/youngchen/Downloads/wiki_inds_normalized', 'rb')\n",
        "data_inds = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file = open('/Users/youngchen/Downloads/random_wiki_sentences_1400_tokens', 'rb')\n",
        "tokens = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file1 = open('/Users/youngchen/Downloads/rand_word_embed_wikitext_1400.pt' , 'rb')\n",
        "embedding = pickle.load(file1)\n",
        "file1.close()\n",
        "whole_embed = torch.reshape(embedding, (91454, 768))\n",
        "whole_embed = whole_embed / np.linalg.norm(whole_embed, axis = 1)[:,None]\n",
        "\n",
        "file1 = open('/Users/youngchen/Downloads/wiki_rand_cluster_assignments_normalized_1440', 'rb')\n",
        "labels = pickle.load(file1)\n",
        "file1.close()\n",
        "\n",
        "file1 = open('/Users/youngchen/Downloads/num_eff_singular_values_rand_1400', 'rb')\n",
        "eff_singular_vals = pickle.load(file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "Guj2_M558s5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = 91454                      # database size\n",
        "nq = 900                       # nb of queries\n",
        "xb = whole_embed.numpy()\n",
        "\n",
        "indice = random.sample(range(91454), 100)\n",
        "indice = torch.tensor(indice)\n",
        "xq = whole_embed[indice]\n",
        "\n",
        "# xq = whole_embed[:10]\n",
        "xq = xq.numpy()\n",
        "\n",
        "index = faiss.IndexFlatL2(768)   # build the index\n",
        "print(index.is_trained)\n",
        "index.add(xb)                  # add vectors to the index\n",
        "print(index.ntotal)"
      ],
      "metadata": {
        "id": "D-rv-iC-86lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 30                       # we want to see the k nearest neighbors\n",
        "D, I = index.search(xb[:10], k) # sanity check\n",
        "print(I.shape)\n",
        "print(D.shape)\n",
        "D, I = index.search(xq, k)     # actual search\n",
        "print(I)                   # neighbors of the queries"
      ],
      "metadata": {
        "id": "ylDRN7VT88Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest = {}\n",
        "\n",
        "for i in range(100):\n",
        "    i1 = xq[i]\n",
        "    proj1, v1, p1 = project_input_to_permutation(i1)\n",
        "    print('The current embed is ' + str(tokens[i]))\n",
        "    \n",
        "    nearest[i] = labels[proj1]\n",
        "\n",
        "print(nearest)"
      ],
      "metadata": {
        "id": "eJyIvxzd9AYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "right_neighbors = 0\n",
        "\n",
        "for k in range(100):\n",
        "    neighbors_faiss = I[k]\n",
        "    for j, value in enumerate(nearest[k]):\n",
        "        if value in neighbors_faiss:\n",
        "            right_neighbors += 1\n",
        "\n",
        "accuracy = right_neighbors / 100 # accuracy metric ;  if one of the 30-NNs are in the cluster, count it as \"accurate\"\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "foK92K6h9VWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}